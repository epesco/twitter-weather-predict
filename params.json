{
  "name": "Twitter Weather Prediction",
  "tagline": "",
  "body": "### Problem Statement\r\nFive hundred million tweets are posted per day via twitter. Twitter is commonly used for sentiment analysis and quantifying influence, but can a tweet be used to predict the weather in a given area? Natural disasters and extreme weather prediction are popular choices for mining twitter data, but few attempts have been made for the day to day weather. This paper explores the possibility of utilizing supervised learning to classify a location’s weather after training on the lexicon associated with a certain weather type. The methods explored are a naive attempt, naive bayes, positive naive bayes, support vector machines, and ensembling. Within each algorithm, we explore the training, construction, and performance of the model. \r\nIn 2013, CrowdFlower created a competition around solving this very task and the winner created a ridge regression model and an ensemble of forests where the output of the ensemble was fed back into the ridge regression model as input to relearn. Our aim is not for accuracy, but for analysis of more simple methods’ performance on this task. Weather prediction using tweets is complex; words are not independent of each other, given a small subset the likelihood of certain words and weather associations are skewed. The methods we use are not suited for a time series problem like weather prediction where the weather of one-time step is dependent on a prior step. More complex and/or time step dependent methods should be considered as more viable options if accuracy is the ultimate goal, but we are interested in exploring the performance of simple, easily accessible algorithms in this space. \r\n\r\n### Data Collection & Processing\r\n#### Collection\r\nWe collected data from the evening of October 4th to the morning of Oct 10th which produced 119, 061 tweets and 135 hours of weather data at half hour intervals.  Using the twitter streaming API, we filtered on location and used the OpenWeatherMap API to check the weather for the city of Boston. The weather data came pre-divided into 9 categories. These were Thunderstorm, Drizzle, Rain, Snow, Clear, Clouds, Extreme, Atmosphere, and Additional. Extreme weather refered to conditions like hurricanes, tornadoes, and hail. Atmosphere included conditions which impair human vision such as fog, dust, and ash. Finally, Additional was for any weather that did not fit into the above categories.\r\nTo combine our data, we iterated through the weather data and produced time ranges of each span of continuous weather and added them to a dictionary where the key was one of the weather types defined by the weather API (9 in total) and the values were tuples of time ranges that the given weather type occurred. \r\n#### Labeling\r\nWe iterated through all the tweets and produced a tuple consisting of a dictionary of the features they contained and the weather label which was found by looking up the time range they fell into and labeled them according to the weather type associated with that time range. A readable example of the labeled data is below:\r\n    ({contains(down.): True, contains(#debate): True, contains(how): True, contains(before): True, contains(the): True, contains(really): True, contains(showing): True, contains(affected): True, contains(crime): True, u'contains(@billclinton)': True, contains(he): True, contains(there): True, contains(personally): True, contains(has): True, contains(debt): True, contains(you\"): True, contains(was): True, contains(houston): True, contains(\"how): True,contains(cared): True, contains(that): True, contains(in): True}, 'Clear')\r\n#### Validation\r\nWe wrote our own cross-validation function which separates the data into n folds and then trains n times. On each iteration the training is conducted on n-1 folds, and the model is then tested on the remaining fold. It tracks the accuracy and training time of the model over each fold iteration and outputs the average accuracy for all iterations.\r\n\r\n### Naive Attempt\r\n#### Overview\r\nIn order to better assess the success of our machine learning algorithms, a naive solution was created to use as a baseline for success. The method which was chosen was a simple keyword comparison. Since the weather data was classified into 9 categories, as discussed in Data Collection, 6 - 10 keywords were identified for each category. For example the keywords “thunderstorm”, “lightning”, “storm\", and “high wind” were all associated with the category of Thunderstorm. To evaluate the predicted conditions of each tweet the number of keywords per category from the text each was recorded. The dominant category would then be the predicted weather conditions when that tweet was sent. However, since many tweets have nothing to do with weather this approach needed to be improved slightly to have any notable success. \r\nTo improve the accuracy of the naive approach, the tweets were bundled into 30-minute increments in order to correspond with each weather data point. To calculate the predicted weather in this 30 minute time block the total keyword count for each weather category was collected. The category with the most keywords would be selected as the prediction. If no keyword was detected in the last half hour then the previous weather was used. This is a reasonable assumption as weather rarely changes drastically over 30 min time steps. \r\n\r\n#### Accuracy\r\nThe accuracy of the naive method was larger than we initially anticipated. The best accuracy recorded was 60.2%. Since, the prediction method was deterministic the algorithm only needed to be run once for each set of keywords on the data. We decided to compare the number of keywords per type with accuracy of the algorithm. The results of these tests can be seen in Figure 1. For all of these tests there was an average running time of 6.68 seconds. \r\n![Figure 1: Accuracy of the naive algorithm vs the number of keywords used](http://i68.tinypic.com/35lrjax.jpg)\r\n\r\nThe results of this experiment point out the biggest flaw with our approach. Since we collected weather data in real time for only about a week, the majority of the data was clear skys. Clear was the starting weather condition of our novice method. Therefore, even if no keywords were present it was still able to achieve 55% accuracy. The other interesting result was how 1-2 keywords resulted in much higher accuracy than larger numbers. This represents how the choice of keywords is far more important than the number used. \r\n\r\n### Naive Bayes\r\n#### Overview\r\nNaive Bayes is a classifier which assumes independence between features. In our instance the equation can be viewed as:\r\nP(weather|words) = P(likelihood of words) * P(weather) / P(evidence) \r\nNote: We don’t need P(evidence) as it is a normalizer and is present in all calculations\r\nNaive bayes attempts to learn the P(weather|words) so that when it receives a new tweet where the words are features, it can predict the most likely weather at the time the tweet was posted.\r\n#### Accuracy\r\nBelow is the accuracy of the classifier across two to ten folds.\r\n![Figure 2: Naive Bayes accuracy vs number of folds](http://i68.tinypic.com/1425wur.jpg)\r\nAcross all folds, the average accuracy of the algorithm in predicting the weather was 59.43%. Over each fold the accuracy seems to be logarithmically increasing toward ~60.5%. Notably, a separate trial of 50 folds resulted in an accuracy of 60.5%.\r\n#### Time\r\n\r\n![Figure 3: Training time per cost validation](http://i63.tinypic.com/143g66h.jpg)\r\nThere was a direct correlation between the number of folds and the time it took to create, train and test the model. The time increased linearly with the increase in folds at a rate of about 24 seconds per fold. The insignificant advancement in accuracy vs time it takes to construct these models alludes to the tradeoff between model accuracy/complexity and practicality.\r\n\r\n#### Most Informative Features\r\nThe most informative features for our final model(s) are listed below:\r\n![Figure 4: The most informative features (words) detected by naive bayes](http://i63.tinypic.com/xn80ls.jpg)\r\nIt is clear that our model thinks whenever there is a presidential debate or SNL is on then it is not clear outside. These models probably wouldn’t perform super well on future data given the weather outside is independent of TV broadcasting (maybe). We were more interested in everyday nuances and colloquial conversations and word choices that could predict the weather as opposed to being overtaken by a few large-scale events. This could have been improved with more data. An interesting set of most informative features came from when we had less data (prior to the debate) which are shown below:\r\n![Figure 5: Particularly interesting informative features when considering current events](http://i65.tinypic.com/2ik2r05.jpg)\r\nThe word coat indicates it is cloudy out with high certainty as well as the word pumpkins. Although I have no explanation for the latter, we aren’t concerned with why a word indicates whether just the fact that it can. However, coat makes sense in terms of what we typically associate with cloudy weather. With more data collected, we would expect to see more frequently used words independent of the major events in the twitter conversation.\r\n\r\n### Positive Naive Bayes\r\n#### Overview\r\nPositive Naive Bayes is a variant of bayes included in the NLTK API, it trains with partially labeled data to perform a binary classification. In this implementation, models were trained for each weather type, and test inputs are then classified as either true or false as having taken place during that specific type of weather. The model requires two data sets to train, a labeled set, and an unlabeled one. The unlabeled set with a random assortment of actual weathers and is used to determine the frequency of the features from the labeled set in general vocabulary. As only three types of weather were observed there were only three models needed. After the inputs are shuffled to create the folds for cross validation, the training data is resorted into lists by known weather type to be used to train the weather models.\r\n\r\n#### Time\r\nPositive Naive Bayes needs to train three models for every iteration, and classify the test set three times. There is only a slight increase in time required for training and testing more folds. The individual time for the three different models is almost identical, and the time for all of them scales to be three times slower. It also requires much lower run time than naive bayes models that include all three types of weather. \r\n\r\n![Figure 6: Positive Naive Bayes training time vs number of folds](http://i67.tinypic.com/fw7hxc.jpg)\r\n\r\n#### Accuracy\r\n![Figure 7: Positive Naive Bayes Accuracy vs number of folds. ](http://i66.tinypic.com/19tdhw.jpg)\r\nTwo metrics were used to calculate the success rate when testing all the models together. In the earlier implementation of Naive Bayes tweets could only be classified as one type of weather, however when trying all models multiple a false positive can occur and tweets could be classified as multiple weather types. Ideally, if there were multiple false positives the certainty for each classification could be evaluated to chose a winner.  However, this was not practical using the NLTK API. For the points, in the figure above, labeled as “Averages: Combined w/ False Positives”  each model contributed to overall accuracy percentage. For each test tweet to receive a full point it needed to be classified correctly three times, and would receive partial credit in the scenario of a false positive. This levels out at a success rate of 48%. In the set of points labeled “Average Combined” awards a point for correctly determining the weather and ignores false positives. This levels out to have a success rate of 56%. Ideally, with an additional evaluation step, false positives could be filtered out correctly and have better than 48% accuracy. \r\nAs the classifier is broken up by weather it is also easier to see how well each type of weather is predicted. The classification for clear days performs the best. During the data collection period, clear weather was seen the most common weather. As a result, both blindly classifying all tweets as ‘clear’ would be the most accurate and there is more training data for ‘Clear’ days with leads to a better model. \r\n\r\n### Support Vector Machine\r\n#### Overview\r\nThe last traditional machine learning method which was employed was a Linear Support Vector Machine (SVM). This method tries to define a decision boundary equidistant between the boundaries of data set categories. This can be seen more clear in the example shown in Figure 8. \r\n![Figure 8: An example of the principle of a support vector machine from Stuart and Norvig](http://i64.tinypic.com/2wnpvkn.jpg)\r\n\r\nThe dotted lines in Figure 8 represent the closest possible boundary of the training data. The SVM algorithm defines the decision boundary as the line equally distant from these boundaries. It might initially seem that SVM is therefore limited to linear models. However, kernels can be applied to nonlinear data to increase its dimensionality. It is then possible to generate linear hyperplanes to divide this high dimensional data. These high dimensional solutions can then be converted back into the original dimension as non linear decision boundaries. Because of their complex and, simultaneously, linear nature SVM’s can generalize well. While SVM’s have reasonable training times they are not nearly as quick as a solution as methods like Naive Bayes (Stuart and Norvig, 2010, p 744 - 748) \r\nWe implemented an SVM using the SVMs available as part of the Scikit-Learn python library. NLTK provides a wrapper for this library which made it possible to integrate it into our code. Three different methods of SVM were evaluated with the most successful one being used in our final algorithm the Ensemble. The tests SVMs were:\r\n1 Linear SVM: This method used a traditional linear kernel for the SVM classifier and compared classifications as one-vs-all others\r\n2 Crammer, Singer Multi Class: This method was the same as Linear SVM except it used a many-vs-many classification\r\n3 Nonlinear SVM: This method used the Radial Basis Function as the kernel allowing the SVM to generate nonlinear decision boundaries. \r\n\r\n#### Accuracy\r\nTo evaluate the accuracy and execution time of each algorithm 5 fold cross validation was used. The averaged results of these tests are shown in Figure 9. \r\n![Figure 9: A comparison of average training time and accuracy between three SVM methods when using 5 fold cross validation. ](http://i67.tinypic.com/sqkxh2.jpg)\r\n\r\nBased on the results traditional Linear SVM was superior to the other methods by far. With both the highest accuracy at 62.5% and the shortest running time at 34.15 seconds. These results were somewhat surprising. While Crammer, Singer performance is consistent with our expectations from research, we expected the more complex RBF kernel algorithms to outperform Linear SVM even if it had a longer running time. It is possible, that there is simply not enough divide amongst the data when using words as features to provide a benefit for complex decision boundaries vs linear ones.\r\n\r\n### Ensembling\r\n#### Overview\r\nIn order to increase the accuracy of our learning algorithms, we decided to combine the predictions from each one in an ensemble. Alone each of our learning algorithms have an accuracy of around 60%, but combining their predictions increases our expected accuracy and confidence. In order to actually combine the algorithms we first trained each one of them separately and then, once training has been completed, new tweets are fed into the ensemble. Each new tweet is classified by each learning algorithm and then the predictions are used to vote for what the ensemble will use as its prediction. For example if the Naive Bayes algorithm’s prediction agrees with that of the SVM algorithm but not with the Positive Naive Bayes algorithm then the vote of 2 to 1 will result in the ensemble’s prediction matching that of the Naive Bayes and SVM algorithms. \r\n#### Accuracy\r\nTo evaluate the accuracy of the ensemble we used different training percentages and equivalent testing percentages in order to find ratio that gave the highest accuracy. The results are shown in the graph below.\r\n![Figure 10: A comparison percentage of total data used as training data vs accuracy for the ensemble.](http://i67.tinypic.com/34ozytu.jpg)\r\nThe accuracy of our ensemble peaks at a value of 62.5 percent at around 90.9 percent training data vs 9.1 percent test data. Any further increase in training data percentage results in a drastic decrease in accuracy. \r\n\r\n### Conclusion\r\nIn conclusion, using Twitter to predict the weather in a given area is possible. However, the accuracy of such a weather prediction system could be greatly increased using more data collected from Twitter. We found that using a Naive Bayes learning algorithm on this kind of data is feasible but the maximum accuracy that we were able to achieve was only 60.5%. Comparing this to using a SVM learning algorithm, which was able to achieve an accuracy of 62.5%, the SM learning algorithm is slightly superior. The downside to the SVM algorithm is that it takes longer to train its model then Naive Bayes. We found that the positive Naive Bayes algorithm was not a good choice for this kind of data, mainly because of the way that we determined features. Needing to train on three different data sets was also inefficient and unnecessary for our purposes. The ensemble of all three of these algorithms was intended to improve the overall accuracy of the predictions when compared to the accuracy that each could achieve by itself. However, the maximum accuracy that we were able to achieve for the ensemble was the same as the maximum accuracy for the SVM algorithm. This may be because more data was needed to see any difference in accuracy, but it is more likely that it is because a greater variety of different algorithms was needed. Positive Naive Bayes and Naive Bayes were most likely too similar to one another to result in any real variance, and SVM is what brought the accuracy of the ensemble up to its maximum. Further recommendations for improving weather prediction using Twitter data is included below.\r\n\r\n### Recommendations\r\n#### Feature Selection\r\nAfter conducting our experiments, it became clear that a major issue with our approach was our feature selection. By using words as features, we restricted our available algorithms to those which worked well with high feature counts. Additionally, it is not clear that word choice itself conveys enough about regional weather. In the future, we suggest that the metadata of the tweets be used instead. This would include such things as retweet frequency, screen name, use of hashtags, etc. Additionally, the implied positivity of the text could be inspected using the Natural Learning Toolkit and used as a feature. This would limit the feature count and expand the number of available algorithms to include methods such as decision trees. Hopefully, this would allow a more accurate algorithm to be developed. \r\n#### Evaluation  Approach\r\nThe approach to predicting weather could have been improved. The methods discussed above try to classify data based on individual tweets. However, as the Naive keyword based approach showed it can be very beneficial to classify weather based on clumps of tweets instead. This is because it is unlikely most tweets are related to the weather. While there might be some hidden trends of how weather affects word choice it is unlikely they are highly pronounced. Therefore, it would be useful to experiment with training on sets of data. Another potential improvement would be to evaluate tweets by source. For example, a tweet from a weather station might be vastly more reliable than a teenager’s. If these and other methods were explored the algorithms could likely be improved. \r\n#### Alternative Algorithms\r\nWhile, we explored the use of 4 different algorithms, other methods are still available. The one most likely to show major improvement is a neural network. The ability of neural nets to deal with complex inputs and data would make them very well suited to this type of classification. Additionally, if the number of features were reduced as described above, then a decision tree could be used. It would be interesting to compare the decision tree with Naive Bayes performance. \r\n#### Data Collection\r\nThe data collection methods used for these experiments could have been vastly improved if given more time. Primarily, spreading tweet data collection over a whole year would have been particularly interesting. This would allow for the data to reflect almost all possible weather scenarios in the region. The major drawback to this method would be the data required. The tweet data from only ~ 1 week took up over 430Mb of space. Over the course of the year this could add up to almost 22Gb of data. Not an unmanageable about but somewhat inconvenient. However, the increased variety of weather would make this compromise more than worth it. \r\n\r\n### Credits & Citations\r\nBird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. O’Reilly Media Inc.\r\n\r\nStuart, R., & Norvig, P. (2010). Artificial Intelligence A Modern Approach (3rd ed.). Upper Saddle River, New Jersey: Pearson Education Inc.\r\n\r\nBuitinck, L., Louppe, G., Blondel, M., Pedregosa, F., Mueller, A., Grisel, O., … Varoquaux, G. (2013). {API} design for machine learning software: experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning (pp. 108–122). inproceedings.\r\n\r\n### Authors and Contributors\r\n@epesco\r\n@msmcconnell\r\n@amjaeger17\r\n@rcjacques\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}