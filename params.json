{
  "name": "Twitter Weather Prediction",
  "tagline": "",
  "body": "### Problem Statement\r\nFive hundred million tweets are posted per day via twitter. Twitter is commonly used for sentiment analysis and \tFive hundred million tweets are posted per day via twitter. Twitter is commonly used for sentiment analysis and quantifying influence, but can a tweet be used to predict the weather in a given area? Natural disasters and extreme weather prediction are popular choices for mining twitter data, but few attempts have been made for the day to day weather. This paper explores the possibility of utilizing supervised learning to classify a location’s weather after training on the lexicon associated with a certain weather type. The methods explored are a naive attempt, naive bayes, positive naive bayes, support vector machines, and ensembling. Within each algorithm, we explore the training, construction, and performance of the model. \r\nIn 2013, CrowdFlower created a competition around solving this very task and the winner created a ridge regression model and an ensemble of forests where the output of the ensemble was fed back into the ridge regression model as input to relearn. Our aim is not for accuracy, but for analysis of more simple methods’ performance on this task. Weather prediction using tweets is complex; words are not independent of each other, given a small subset the likelihood of certain words and weather associations are skewed. The methods we use are not suited for a time series problem like weather prediction where the weather of one time step is dependent on a prior step. More complex and/or time step dependent methods should be considered as more viable options if accuracy is the ultimate goal, but we are interested in exploring the performance of simple, easily accessible algorithms in this space. \r\n\r\n### DATA COLLECTION & PROCESSING\r\n#### Collection\r\nWe collected data from the evening of October 4th to the morning of Oct 10th which produced 119, 061 tweets and 135 hours of weather data at half hour intervals.  Using the twitter streaming API, we filtered on location and used the OpenWeatherMap API to check weather for city of Boston. The weather data came pre-divided into 9 categories. These were Thunderstorm, Drizzle, Rain, Snow, Clear, Clouds, Extreme, Atmosphere, and Additional. Extreme weather refered to conditions like hurricanes, tornadoes, and hail. Atmosphere included conditions which impair human vision such as fog, dust, and ash. Finally, Additional was for any weather that did not fit into the above categories.\r\nTo combine our data, we iterated through the weather data and produced time ranges of each span of continuous weather and added them to a dictionary where the key was one of the weather types defined by the weather API (9 in total) and the values were tuples of time ranges that the given weather type occurred. \r\n#### Labeling\r\nWe iterated through all the tweets and produced a tuple consisting of a dictionary of the features they contained and the weather label which was found by looking up the time range they fell into and labeled them according to the weather type associated with that time range. A readable example of the labeled data is below:\r\n    ({contains(down.): True, contains(#debate): True, contains(how): True, contains(before): True, contains(the): True, contains(really): True, contains(showing): True, contains(affected): True, contains(crime): True, u'contains(@billclinton)': True, contains(he): True, contains(there): True, contains(personally): True, contains(has): True, contains(debt): True, contains(you\"): True, contains(was): True, contains(houston): True, contains(\"how): True,contains(cared): True, contains(that): True, contains(in): True}, 'Clear')\r\n#### Validation\r\nWe wrote our own cross-validation function which separates the data into n folds and then trains n times. On each iteration the training is conducted on n-1 folds, and the model is then tested on the remaining fold. It tracks the accuracy and training time of the model over each fold iteration and outputs the average accuracy for all iterations.\r\n\r\n### Naive Attempt\r\n#### Overview\r\nIn order to better assess the success of our machine learning algorithms, a naive solution was created to use as a baseline for success. The method which was chosen was a simple keyword comparison. Since the weather data was classified into 9 categories, as discussed in Data Collection, 6 - 10 keywords were identified for each category. For example the keywords “thunderstorm”, “lightning”, “storm\", and “high wind” were all associated with the category of Thunderstorm. To evaluate the predicted conditions of each tweet the number of keywords per category from the text each was recorded. The dominant category would then be the predicted weather conditions when that tweet was sent. However, since many tweets have nothing to do with weather this approach needed to be improved slightly to have any notable success. \r\nTo improve the accuracy of the naive approach, the tweets were bundled into 30-minute increments in order to correspond with each weather data point. To calculate the predicted weather in this 30 minute time block the total keyword count for each weather category was collected. The category with the most keywords would be selected as the prediction. If no keyword was detected in the last half hour then the previous weather was used. This is a reasonable assumption as weather rarely changes drastically over 30 min time steps. \r\n\r\n#### Accuracy\r\nThe accuracy of the naive method was larger than we initially anticipated. The best accuracy recorded was 60.2%. Since, the prediction method was deterministic the algorithm only needed to be run once for each set of keywords on the data. We decided to compare the number of keywords per type with accuracy of the algorithm. The results of these tests can be seen in Figure 1. For all of these tests there was an average running time of 6.68 seconds. \r\n![Figure 1: Accuracy of the naive algorithm vs the number of keywords used](http://i68.tinypic.com/35lrjax.jpg)\r\n\r\nThe results of this experiment point out the biggest flaw with our approach. Since we collected weather data in real time for only about a week, the majority of the data was clear skys. Clear was the starting weather condition of our novice method. Therefore, even if no keywords were present it was still able to achieve 55% accuracy. The other interesting result was how 1-2 keywords resulted in much higher accuracy than larger numbers. This represents how the choice of keywords is far more important than the number used. \r\n\r\n### Naive Bayes\r\n#### Overview\r\nNaive Bayes is a classifier which assumes independence between features. In our instance the equation can be viewed as:\r\nP(weather|words) = P(likelihood of words) * P(weather) / P(evidence) \r\nNote: We don’t need P(evidence) as it is a normalizer and is present in all calculations\r\nNaive bayes attempts to learn the P(weather|words) so that when it receives a new tweet where the words are features, it can predict the most likely weather at the time the tweet was posted.\r\n#### Accuracy\r\nBelow is the accuracy of the classifier across two to ten folds.\r\n![Figure 2: Naive Bayes accuracy vs number of folds](http://i68.tinypic.com/1425wur.jpg)\r\nAcross all folds, the average accuracy of the algorithm in predicting the weather was 59.43%. Over each fold the accuracy seems to be logarithmically increasing toward ~60.5%. Notably, a separate trial of 50 folds resulted in an accuracy of 60.5%.\r\n#### Time\r\n\r\n![Figure 3: Training time per cost validation](http://i63.tinypic.com/143g66h.jpg)\r\nThere was a direct correlation between the number of folds and the time it took to create, train and test the model. The time increased linearly with the increase in folds at a rate of about 24 seconds per fold. The insignificant advancement in accuracy vs time it takes to construct these models alludes to the tradeoff between model accuracy/complexity and practicality.\r\n\r\n#### Most Informative Features\r\nThe most informative features for our final model(s) are listed below:\r\n![Figure 4: The most informative features (words) detected by naive bayes](http://i63.tinypic.com/xn80ls.jpg)\r\nIt is clear that our model thinks whenever there is a presidential debate or SNL is on then it is not clear outside. These models probably wouldn’t perform super well on future data given the weather outside is independent of TV broadcasting (maybe). We were more interested in everyday nuances and colloquial conversations and word choices that could predict the weather as opposed to being overtaken by a few large-scale events. This could have been improved with more data. An interesting set of most informative features came from when we had less data (prior to the debate) which are shown below:\r\n![Figure 5: Particularly interesting informative features when considering current events](http://i65.tinypic.com/2ik2r05.jpg)\r\nThe word coat indicates it is cloudy out with high certainty as well as the word pumpkins. Although I have no explanation for the latter, we aren’t concerned with why a word indicates whether just the fact that it can. However, coat makes sense in terms of what we typically associate with cloudy weather. With more data collected, we would expect to see more frequently used words independent of the major events in the twitter conversation.\r\n\r\n### POSITIVE NAIVE BAYES\r\n#### Overview\r\nPositive Naive Bayes is a variant of bayes included in the NLTK API, it trains with partially labeled data to perform a binary classification. In this implementation, models were trained for each weather type, and test inputs are then classified as either true or false as having taken place during that specific type of weather. The model requires two data sets to train, a labeled set, and an unlabeled one. The unlabeled set with a random assortment of actual weathers and is used to determine the frequency of the features from the labeled set in general vocabulary. As only three types of weather were observed there were only three models needed. After the inputs are shuffled to create the folds for cross validation, the training data is resorted into lists by known weather type to be used to train the weather models.\r\n\r\n#### Time\r\nPositive Naive Bayes needs to train three models for every iteration, and classify the test set three times. There is only a slight increase in time required for training and testing more folds. The individual time for the three different models is almost identical, and the time for all of them scales to be three times slower. It also requires much lower run time than naive bayes models that include all three types of weather. \r\n![Figure 6: Positive Naive Bayes training time vs number of folds](http://i67.tinypic.com/fw7hxc.jpg)\r\n\r\n### Authors and Contributors\r\n\r\n@msmcconnell\r\n@amjaeger17\r\n@rcjacques\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}